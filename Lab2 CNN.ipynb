{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Student 1:</b> Benigmim Mohammed Yasser\n",
    "<b> Student 2:</b> Lopez Colombe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "Follow instructions step by step until the end and submit your complete notebook as an archive (tar -cf groupXnotebook.tar DL_lab2/).\n",
    "\n",
    "Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by May 29th 2019 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%.  Can  you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to  more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288720207\n",
      "Epoch:  02   =====> Loss= 0.732706248\n",
      "Epoch:  03   =====> Loss= 0.600367205\n",
      "Epoch:  04   =====> Loss= 0.536793501\n",
      "Epoch:  05   =====> Loss= 0.497855682\n",
      "Epoch:  06   =====> Loss= 0.471093028\n",
      "Epoch:  07   =====> Loss= 0.450922387\n",
      "Epoch:  08   =====> Loss= 0.435924568\n",
      "Epoch:  09   =====> Loss= 0.423351202\n",
      "Epoch:  10   =====> Loss= 0.413254377\n",
      "Epoch:  11   =====> Loss= 0.404583712\n",
      "Epoch:  12   =====> Loss= 0.396899246\n",
      "Epoch:  13   =====> Loss= 0.390303331\n",
      "Epoch:  14   =====> Loss= 0.384377501\n",
      "Epoch:  15   =====> Loss= 0.379212090\n",
      "Epoch:  16   =====> Loss= 0.374694992\n",
      "Epoch:  17   =====> Loss= 0.370271073\n",
      "Epoch:  18   =====> Loss= 0.366335135\n",
      "Epoch:  19   =====> Loss= 0.362733373\n",
      "Epoch:  20   =====> Loss= 0.359501061\n",
      "Epoch:  21   =====> Loss= 0.356150374\n",
      "Epoch:  22   =====> Loss= 0.354020533\n",
      "Epoch:  23   =====> Loss= 0.351282963\n",
      "Epoch:  24   =====> Loss= 0.348624653\n",
      "Epoch:  25   =====> Loss= 0.346463552\n",
      "Epoch:  26   =====> Loss= 0.344494880\n",
      "Epoch:  27   =====> Loss= 0.342271795\n",
      "Epoch:  28   =====> Loss= 0.340085938\n",
      "Epoch:  29   =====> Loss= 0.338369405\n",
      "Epoch:  30   =====> Loss= 0.336629907\n",
      "Epoch:  31   =====> Loss= 0.335060425\n",
      "Epoch:  32   =====> Loss= 0.333482659\n",
      "Epoch:  33   =====> Loss= 0.332122919\n",
      "Epoch:  34   =====> Loss= 0.330579677\n",
      "Epoch:  35   =====> Loss= 0.329107387\n",
      "Epoch:  36   =====> Loss= 0.327780957\n",
      "Epoch:  37   =====> Loss= 0.326329540\n",
      "Epoch:  38   =====> Loss= 0.325528886\n",
      "Epoch:  39   =====> Loss= 0.324328261\n",
      "Epoch:  40   =====> Loss= 0.322997237\n",
      "Optimization Finished!\n",
      "Accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\" width=\"800\" height=\"600\" align=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "def create_conv(prev, filter_size, nb, padding):#fonction to create the convolution layer\n",
    "    #Set model weights\n",
    "    conv_W = weight_variable((filter_size, filter_size, int(prev.get_shape()[-1]), nb))\n",
    "    conv_b = bias_variable([nb])\n",
    "    #creation of the convolution layer\n",
    "    conv   = tf.nn.conv2d(prev, conv_W, strides=[1, 1, 1, 1], padding=padding) + conv_b\n",
    "    # Activation: relu\n",
    "    conv = tf.nn.relu(conv)\n",
    "    # Pooling\n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding=padding) #\"VALID\" ?? (c'est ce qu'il se fait généralement)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def LeNet5_Model(image):    \n",
    "    # your inmplementation goes here\n",
    "    conv = create_conv(image,5,6,\"SAME\") #zero padding\n",
    "    conv = create_conv(conv,5,16,\"VALID\") #without padding\n",
    "    flat=flatten(conv)\n",
    "\n",
    "    # First fully connected layer\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(int(flat.get_shape()[1]), 120)))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(flat, fc1_W) + fc1_b\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    #Second fully connected layer\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(int(fc1.get_shape()[1]), 84)))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2   = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    #Third fully connected layer\n",
    "    fc3_W = tf.Variable(tf.truncated_normal(shape=(int(fc2.get_shape()[1]), 10)))\n",
    "    fc3_b = tf.Variable(tf.zeros(10))\n",
    "    fc3   = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    logits = tf.nn.softmax(fc3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 28,28,1], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeNet5_Model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Weights:0' shape=(784, 10) dtype=float32_ref>\n",
      "<tf.Variable 'Bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable:0' shape=(5, 5, 1, 6) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(6,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_2:0' shape=(5, 5, 6, 16) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_3:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_4:0' shape=(400, 120) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_5:0' shape=(120,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_6:0' shape=(120, 84) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_7:0' shape=(84,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_8:0' shape=(84, 10) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_9:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7840\n",
      "10\n",
      "150\n",
      "6\n",
      "2400\n",
      "16\n",
      "48000\n",
      "120\n",
      "10080\n",
      "84\n",
      "840\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    print(np.prod(v.get_shape().as_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69556"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Answer</h3>\n",
    "<h4>Convolutionnal layers</h4>\n",
    "<li>The first convolution layer has 6 filters (the depth of the output is equal to the number of filters) of size 5x5. It takes an input of depth one (there is only one grayscale image). Thus,<br>\n",
    "    -the number of parameters for the weights is 5x5x1x6=150 <br>\n",
    "    -the number of parameters for the biases is 6<br>\n",
    "    =>Therefore, this layer has 150+6 = <b>156</b> parameters.</li>\n",
    "The pooling layer doesn't have any parameter.\n",
    "<li>The second convolution layer has 16 filters of size 5x5. The depth of the inputs is the depth of the outputs of the previous layer, thus the depth of the inputs is 6. Thus,<br>\n",
    "    -the number of parameters for the weights is 5x5x6x16=2400 <br>\n",
    "    -the number of parameters for the biases is 16.<br>\n",
    "    Therefore, this layer has 2400+16 = <b>2416</b> parameters.</li>\n",
    "As before, the pooling layer doesn't have any parameter by definition.\n",
    "<h4>Fully connected layers</h4>\n",
    "The number of parameters for fully connnected layers is (number_of_inputs+1)xnumber_of_outputs.\n",
    "<li>After the flatten step, there will be 5x5x16 inputs in the first fully connected layer. Also, there are 120 outputs from this layer. Thus, this layer will have (5x5x16+1)x120 = <b>48120</b> parameters.</li>\n",
    "<li>The second fully connected layer has 120 inputs and 84 outputs. Thus, this layer will have (120+1)x84 = <b>10164</b> parameters.</li>\n",
    "<li>The last fully connected layer has 84 inputs and 10 outputs. Thus, it will have (84+1)x10 = <b>850</b> parameters.</li><BR>\n",
    "<li><b>Finally, by summing the number of parameters of all the layers, we end up with a total of 61 706 parameters for this model.</b></li>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61706\n"
     ]
    }
   ],
   "source": [
    "a = 5*5*6+6\n",
    "b =  5*5*16*6+16\n",
    "c = (16 *5*5+1) *120\n",
    "d = (120+1)*84\n",
    "e = (84+1)*10\n",
    "total = a +b +c +d +e\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NB</h4>\n",
    "The two first Variables \"Weights:0\" and \"Biases:0\" come from the algorithm implemented in Section 1 (we had 28x28x10=7840 weights and 10 biases).<br>\n",
    "Thus, the number of parameters corresponding to our model is 69556-7840-10=61 706."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"x\")\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name=\"y\")\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = LeNet5_Model(x) # The LeNet architecture we implemented previously\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(logits, labels):\n",
    "    # logits will be the outputs of your model, labels will be one-hot vectors corresponding to the actual labels\n",
    "    # logits and labels are numpy arrays\n",
    "    # this function should return the accuracy of your model\n",
    "    predicted_number = tf.argmax(logits, axis=1)\n",
    "    correct_prediction = tf.equal(predicted_number, tf.argmax(labels, axis=1))    \n",
    "    accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy_operation\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc=evaluate(pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01 mini batch:  00   =====> Loss= 21.635761261\n",
      "Epoch:  01 mini batch:  100   =====> Loss= 2.294750929\n",
      "Epoch:  01 mini batch:  200   =====> Loss= 2.208283424\n",
      "Epoch:  01 mini batch:  300   =====> Loss= 2.072838306\n",
      "Epoch:  01 mini batch:  400   =====> Loss= 2.091166496\n",
      "Epoch:  01   =====> Loss= 4.913833133 ,   Train accuracy= 0.2578125 ,   Validation accuracy= 0.222\n",
      "Epoch:  02 mini batch:  00   =====> Loss= 2.053969383\n",
      "Epoch:  02 mini batch:  100   =====> Loss= 1.904825211\n",
      "Epoch:  02 mini batch:  200   =====> Loss= 1.896517754\n",
      "Epoch:  02 mini batch:  300   =====> Loss= 2.076156139\n",
      "Epoch:  02 mini batch:  400   =====> Loss= 2.258590460\n",
      "Epoch:  02   =====> Loss= 1.902920057 ,   Train accuracy= 0.4609375 ,   Validation accuracy= 0.455\n",
      "Epoch:  03 mini batch:  00   =====> Loss= 1.689344168\n",
      "Epoch:  03 mini batch:  100   =====> Loss= 1.500219822\n",
      "Epoch:  03 mini batch:  200   =====> Loss= 1.910304308\n",
      "Epoch:  03 mini batch:  300   =====> Loss= 1.576107502\n",
      "Epoch:  03 mini batch:  400   =====> Loss= 1.475889444\n",
      "Epoch:  03   =====> Loss= 1.714037204 ,   Train accuracy= 0.484375 ,   Validation accuracy= 0.5054\n",
      "Epoch:  04 mini batch:  00   =====> Loss= 1.666288137\n",
      "Epoch:  04 mini batch:  100   =====> Loss= 1.453974962\n",
      "Epoch:  04 mini batch:  200   =====> Loss= 1.893870592\n",
      "Epoch:  04 mini batch:  300   =====> Loss= 1.870001793\n",
      "Epoch:  04 mini batch:  400   =====> Loss= 1.878803015\n",
      "Epoch:  04   =====> Loss= 1.638620075 ,   Train accuracy= 0.328125 ,   Validation accuracy= 0.3356\n",
      "Epoch:  05 mini batch:  00   =====> Loss= 1.756446362\n",
      "Epoch:  05 mini batch:  100   =====> Loss= 1.755275249\n",
      "Epoch:  05 mini batch:  200   =====> Loss= 2.105933189\n",
      "Epoch:  05 mini batch:  300   =====> Loss= 1.351333141\n",
      "Epoch:  05 mini batch:  400   =====> Loss= 1.208787322\n",
      "Epoch:  05   =====> Loss= 1.615171195 ,   Train accuracy= 0.6875 ,   Validation accuracy= 0.59\n",
      "Epoch:  06 mini batch:  00   =====> Loss= 1.267178535\n",
      "Epoch:  06 mini batch:  100   =====> Loss= 1.276440859\n",
      "Epoch:  06 mini batch:  200   =====> Loss= 1.197738528\n",
      "Epoch:  06 mini batch:  300   =====> Loss= 1.325760841\n",
      "Epoch:  06 mini batch:  400   =====> Loss= 1.107591867\n",
      "Epoch:  06   =====> Loss= 1.339544127 ,   Train accuracy= 0.625 ,   Validation accuracy= 0.6046\n",
      "Epoch:  07 mini batch:  00   =====> Loss= 1.525092363\n",
      "Epoch:  07 mini batch:  100   =====> Loss= 1.312881947\n",
      "Epoch:  07 mini batch:  200   =====> Loss= 1.322483659\n",
      "Epoch:  07 mini batch:  300   =====> Loss= 1.099246383\n",
      "Epoch:  07 mini batch:  400   =====> Loss= 1.108980536\n",
      "Epoch:  07   =====> Loss= 1.254979564 ,   Train accuracy= 0.6171875 ,   Validation accuracy= 0.6492\n",
      "Epoch:  08 mini batch:  00   =====> Loss= 0.948328733\n",
      "Epoch:  08 mini batch:  100   =====> Loss= 1.040363431\n",
      "Epoch:  08 mini batch:  200   =====> Loss= 1.316888094\n",
      "Epoch:  08 mini batch:  300   =====> Loss= 0.963963866\n",
      "Epoch:  08 mini batch:  400   =====> Loss= 1.208387256\n",
      "Epoch:  08   =====> Loss= 1.142354227 ,   Train accuracy= 0.546875 ,   Validation accuracy= 0.5288\n",
      "Epoch:  09 mini batch:  00   =====> Loss= 1.669808745\n",
      "Epoch:  09 mini batch:  100   =====> Loss= 1.043759704\n",
      "Epoch:  09 mini batch:  200   =====> Loss= 1.102985978\n",
      "Epoch:  09 mini batch:  300   =====> Loss= 1.033279657\n",
      "Epoch:  09 mini batch:  400   =====> Loss= 0.925274014\n",
      "Epoch:  09   =====> Loss= 1.027174425 ,   Train accuracy= 0.484375 ,   Validation accuracy= 0.5782\n",
      "Epoch:  10 mini batch:  00   =====> Loss= 1.259480715\n",
      "Epoch:  10 mini batch:  100   =====> Loss= 1.261020064\n",
      "Epoch:  10 mini batch:  200   =====> Loss= 0.949769914\n",
      "Epoch:  10 mini batch:  300   =====> Loss= 0.910210729\n",
      "Epoch:  10 mini batch:  400   =====> Loss= 0.847391069\n",
      "Epoch:  10   =====> Loss= 0.958297923 ,   Train accuracy= 0.5703125 ,   Validation accuracy= 0.5652\n",
      "Epoch:  11 mini batch:  00   =====> Loss= 1.285645962\n",
      "Epoch:  11 mini batch:  100   =====> Loss= 0.889060199\n",
      "Epoch:  11 mini batch:  200   =====> Loss= 0.809013844\n",
      "Epoch:  11 mini batch:  300   =====> Loss= 1.178100944\n",
      "Epoch:  11 mini batch:  400   =====> Loss= 0.684691787\n",
      "Epoch:  11   =====> Loss= 0.885207322 ,   Train accuracy= 0.8203125 ,   Validation accuracy= 0.772\n",
      "Epoch:  12 mini batch:  00   =====> Loss= 0.862147868\n",
      "Epoch:  12 mini batch:  100   =====> Loss= 1.503944039\n",
      "Epoch:  12 mini batch:  200   =====> Loss= 0.744957089\n",
      "Epoch:  12 mini batch:  300   =====> Loss= 0.679960728\n",
      "Epoch:  12 mini batch:  400   =====> Loss= 0.777096510\n",
      "Epoch:  12   =====> Loss= 0.847076186 ,   Train accuracy= 0.7890625 ,   Validation accuracy= 0.7736\n",
      "Epoch:  13 mini batch:  00   =====> Loss= 0.620289981\n",
      "Epoch:  13 mini batch:  100   =====> Loss= 0.909181952\n",
      "Epoch:  13 mini batch:  200   =====> Loss= 0.657598138\n",
      "Epoch:  13 mini batch:  300   =====> Loss= 0.591133893\n",
      "Epoch:  13 mini batch:  400   =====> Loss= 0.650715828\n",
      "Epoch:  13   =====> Loss= 0.778307741 ,   Train accuracy= 0.828125 ,   Validation accuracy= 0.7988\n",
      "Epoch:  14 mini batch:  00   =====> Loss= 0.623450279\n",
      "Epoch:  14 mini batch:  100   =====> Loss= 0.760216177\n",
      "Epoch:  14 mini batch:  200   =====> Loss= 0.558661819\n",
      "Epoch:  14 mini batch:  300   =====> Loss= 1.125516176\n",
      "Epoch:  14 mini batch:  400   =====> Loss= 0.619011283\n",
      "Epoch:  14   =====> Loss= 0.793494385 ,   Train accuracy= 0.78125 ,   Validation accuracy= 0.8018\n",
      "Epoch:  15 mini batch:  00   =====> Loss= 0.691731930\n",
      "Epoch:  15 mini batch:  100   =====> Loss= 0.598945379\n",
      "Epoch:  15 mini batch:  200   =====> Loss= 0.786882520\n",
      "Epoch:  15 mini batch:  300   =====> Loss= 0.925650239\n",
      "Epoch:  15 mini batch:  400   =====> Loss= 0.524433374\n",
      "Epoch:  15   =====> Loss= 0.720201928 ,   Train accuracy= 0.7734375 ,   Validation accuracy= 0.8074\n",
      "Epoch:  16 mini batch:  00   =====> Loss= 0.735460997\n",
      "Epoch:  16 mini batch:  100   =====> Loss= 1.080059052\n",
      "Epoch:  16 mini batch:  200   =====> Loss= 0.877739489\n",
      "Epoch:  16 mini batch:  300   =====> Loss= 0.717451453\n",
      "Epoch:  16 mini batch:  400   =====> Loss= 0.739120603\n",
      "Epoch:  16   =====> Loss= 0.669606661 ,   Train accuracy= 0.8046875 ,   Validation accuracy= 0.831\n",
      "Epoch:  17 mini batch:  00   =====> Loss= 0.718284965\n",
      "Epoch:  17 mini batch:  100   =====> Loss= 1.014075160\n",
      "Epoch:  17 mini batch:  200   =====> Loss= 0.497809142\n",
      "Epoch:  17 mini batch:  300   =====> Loss= 0.319350779\n",
      "Epoch:  17 mini batch:  400   =====> Loss= 0.609967589\n",
      "Epoch:  17   =====> Loss= 0.631403498 ,   Train accuracy= 0.8125 ,   Validation accuracy= 0.84\n",
      "Epoch:  18 mini batch:  00   =====> Loss= 0.651593566\n",
      "Epoch:  18 mini batch:  100   =====> Loss= 0.687133133\n",
      "Epoch:  18 mini batch:  200   =====> Loss= 0.460464299\n",
      "Epoch:  18 mini batch:  300   =====> Loss= 0.511774659\n",
      "Epoch:  18 mini batch:  400   =====> Loss= 0.563899279\n",
      "Epoch:  18   =====> Loss= 0.595755041 ,   Train accuracy= 0.890625 ,   Validation accuracy= 0.8338\n",
      "Epoch:  19 mini batch:  00   =====> Loss= 0.514962375\n",
      "Epoch:  19 mini batch:  100   =====> Loss= 2.082902431\n",
      "Epoch:  19 mini batch:  200   =====> Loss= 0.650947452\n",
      "Epoch:  19 mini batch:  300   =====> Loss= 0.511414289\n",
      "Epoch:  19 mini batch:  400   =====> Loss= 0.588285208\n",
      "Epoch:  19   =====> Loss= 0.583145211 ,   Train accuracy= 0.8671875 ,   Validation accuracy= 0.8564\n",
      "Epoch:  20 mini batch:  00   =====> Loss= 0.363559544\n",
      "Epoch:  20 mini batch:  100   =====> Loss= 0.420952260\n",
      "Epoch:  20 mini batch:  200   =====> Loss= 0.591519952\n",
      "Epoch:  20 mini batch:  300   =====> Loss= 0.406912148\n",
      "Epoch:  20 mini batch:  400   =====> Loss= 0.566859722\n",
      "Epoch:  20   =====> Loss= 0.551457049 ,   Train accuracy= 0.828125 ,   Validation accuracy= 0.8524\n",
      "Epoch:  21 mini batch:  00   =====> Loss= 0.345732212\n",
      "Epoch:  21 mini batch:  100   =====> Loss= 0.443578422\n",
      "Epoch:  21 mini batch:  200   =====> Loss= 0.516836405\n",
      "Epoch:  21 mini batch:  300   =====> Loss= 0.511496067\n",
      "Epoch:  21 mini batch:  400   =====> Loss= 0.638164401\n",
      "Epoch:  21   =====> Loss= 0.526705762 ,   Train accuracy= 0.828125 ,   Validation accuracy= 0.8618\n",
      "Epoch:  22 mini batch:  00   =====> Loss= 0.521200240\n",
      "Epoch:  22 mini batch:  100   =====> Loss= 0.779909432\n",
      "Epoch:  22 mini batch:  200   =====> Loss= 0.501261711\n",
      "Epoch:  22 mini batch:  300   =====> Loss= 0.681030750\n",
      "Epoch:  22 mini batch:  400   =====> Loss= 0.656955123\n",
      "Epoch:  22   =====> Loss= 0.509839149 ,   Train accuracy= 0.8984375 ,   Validation accuracy= 0.8756\n",
      "Epoch:  23 mini batch:  00   =====> Loss= 0.444839537\n",
      "Epoch:  23 mini batch:  100   =====> Loss= 0.572884977\n",
      "Epoch:  23 mini batch:  200   =====> Loss= 0.469996244\n",
      "Epoch:  23 mini batch:  300   =====> Loss= 0.563819647\n",
      "Epoch:  23 mini batch:  400   =====> Loss= 0.488285333\n",
      "Epoch:  23   =====> Loss= 0.496299965 ,   Train accuracy= 0.8828125 ,   Validation accuracy= 0.8596\n",
      "Epoch:  24 mini batch:  00   =====> Loss= 0.465422750\n",
      "Epoch:  24 mini batch:  100   =====> Loss= 0.450930357\n",
      "Epoch:  24 mini batch:  200   =====> Loss= 0.525298476\n",
      "Epoch:  24 mini batch:  300   =====> Loss= 0.540403306\n",
      "Epoch:  24 mini batch:  400   =====> Loss= 0.345552444\n",
      "Epoch:  24   =====> Loss= 0.476373792 ,   Train accuracy= 0.8828125 ,   Validation accuracy= 0.8684\n",
      "Epoch:  25 mini batch:  00   =====> Loss= 0.398968965\n",
      "Epoch:  25 mini batch:  100   =====> Loss= 0.445826769\n",
      "Epoch:  25 mini batch:  200   =====> Loss= 0.309609413\n",
      "Epoch:  25 mini batch:  300   =====> Loss= 0.495541334\n",
      "Epoch:  25 mini batch:  400   =====> Loss= 0.306527972\n",
      "Epoch:  25   =====> Loss= 0.457452267 ,   Train accuracy= 0.9140625 ,   Validation accuracy= 0.88\n",
      "Epoch:  26 mini batch:  00   =====> Loss= 0.452090025\n",
      "Epoch:  26 mini batch:  100   =====> Loss= 0.592467487\n",
      "Epoch:  26 mini batch:  200   =====> Loss= 0.281995833\n",
      "Epoch:  26 mini batch:  300   =====> Loss= 0.474439830\n",
      "Epoch:  26 mini batch:  400   =====> Loss= 0.339332879\n",
      "Epoch:  26   =====> Loss= 0.461567020 ,   Train accuracy= 0.9140625 ,   Validation accuracy= 0.8876\n",
      "Epoch:  27 mini batch:  00   =====> Loss= 0.327963114\n",
      "Epoch:  27 mini batch:  100   =====> Loss= 0.642418265\n",
      "Epoch:  27 mini batch:  200   =====> Loss= 0.396778762\n",
      "Epoch:  27 mini batch:  300   =====> Loss= 0.366508782\n",
      "Epoch:  27 mini batch:  400   =====> Loss= 0.270807803\n",
      "Epoch:  27   =====> Loss= 0.444080328 ,   Train accuracy= 0.8828125 ,   Validation accuracy= 0.8884\n",
      "Epoch:  28 mini batch:  00   =====> Loss= 0.289912462\n",
      "Epoch:  28 mini batch:  100   =====> Loss= 0.514965296\n",
      "Epoch:  28 mini batch:  200   =====> Loss= 0.422563553\n",
      "Epoch:  28 mini batch:  300   =====> Loss= 0.355859995\n",
      "Epoch:  28 mini batch:  400   =====> Loss= 0.440646917\n",
      "Epoch:  28   =====> Loss= 0.432877460 ,   Train accuracy= 0.890625 ,   Validation accuracy= 0.8978\n",
      "Epoch:  29 mini batch:  00   =====> Loss= 0.381961763\n",
      "Epoch:  29 mini batch:  100   =====> Loss= 0.314566970\n",
      "Epoch:  29 mini batch:  200   =====> Loss= 0.596602798\n",
      "Epoch:  29 mini batch:  300   =====> Loss= 0.450058341\n",
      "Epoch:  29 mini batch:  400   =====> Loss= 0.389316916\n",
      "Epoch:  29   =====> Loss= 0.429339593 ,   Train accuracy= 0.90625 ,   Validation accuracy= 0.8996\n",
      "Epoch:  30 mini batch:  00   =====> Loss= 0.436244011\n",
      "Epoch:  30 mini batch:  100   =====> Loss= 0.325677693\n",
      "Epoch:  30 mini batch:  200   =====> Loss= 0.354597628\n",
      "Epoch:  30 mini batch:  300   =====> Loss= 0.526655972\n",
      "Epoch:  30 mini batch:  400   =====> Loss= 0.280108750\n",
      "Epoch:  30   =====> Loss= 0.413440562 ,   Train accuracy= 0.8671875 ,   Validation accuracy= 0.8932\n",
      "Epoch:  31 mini batch:  00   =====> Loss= 0.455868244\n",
      "Epoch:  31 mini batch:  100   =====> Loss= 0.366677880\n",
      "Epoch:  31 mini batch:  200   =====> Loss= 0.569924712\n",
      "Epoch:  31 mini batch:  300   =====> Loss= 0.508924603\n",
      "Epoch:  31 mini batch:  400   =====> Loss= 0.404352307\n",
      "Epoch:  31   =====> Loss= 0.408378362 ,   Train accuracy= 0.8515625 ,   Validation accuracy= 0.9014\n",
      "Epoch:  32 mini batch:  00   =====> Loss= 0.467396796\n",
      "Epoch:  32 mini batch:  100   =====> Loss= 0.529635191\n",
      "Epoch:  32 mini batch:  200   =====> Loss= 0.354341626\n",
      "Epoch:  32 mini batch:  300   =====> Loss= 0.516331375\n",
      "Epoch:  32 mini batch:  400   =====> Loss= 0.441706091\n",
      "Epoch:  32   =====> Loss= 0.399658409 ,   Train accuracy= 0.8671875 ,   Validation accuracy= 0.861\n",
      "Epoch:  33 mini batch:  00   =====> Loss= 0.554795504\n",
      "Epoch:  33 mini batch:  100   =====> Loss= 0.374355644\n",
      "Epoch:  33 mini batch:  200   =====> Loss= 0.399720728\n",
      "Epoch:  33 mini batch:  300   =====> Loss= 0.296310812\n",
      "Epoch:  33 mini batch:  400   =====> Loss= 0.197911978\n",
      "Epoch:  33   =====> Loss= 0.396701395 ,   Train accuracy= 0.6015625 ,   Validation accuracy= 0.5888\n",
      "Epoch:  34 mini batch:  00   =====> Loss= 1.126171708\n",
      "Epoch:  34 mini batch:  100   =====> Loss= 0.373752594\n",
      "Epoch:  34 mini batch:  200   =====> Loss= 0.241344243\n",
      "Epoch:  34 mini batch:  300   =====> Loss= 0.269144416\n",
      "Epoch:  34 mini batch:  400   =====> Loss= 0.284692913\n",
      "Epoch:  34   =====> Loss= 0.382370167 ,   Train accuracy= 0.8984375 ,   Validation accuracy= 0.903\n",
      "Epoch:  35 mini batch:  00   =====> Loss= 0.219996512\n",
      "Epoch:  35 mini batch:  100   =====> Loss= 0.350903332\n",
      "Epoch:  35 mini batch:  200   =====> Loss= 0.361628711\n",
      "Epoch:  35 mini batch:  300   =====> Loss= 0.343035370\n",
      "Epoch:  35 mini batch:  400   =====> Loss= 0.419065177\n",
      "Epoch:  35   =====> Loss= 0.365723895 ,   Train accuracy= 0.921875 ,   Validation accuracy= 0.901\n",
      "Epoch:  36 mini batch:  00   =====> Loss= 0.254309803\n",
      "Epoch:  36 mini batch:  100   =====> Loss= 0.270082563\n",
      "Epoch:  36 mini batch:  200   =====> Loss= 0.409454167\n",
      "Epoch:  36 mini batch:  300   =====> Loss= 0.357302070\n",
      "Epoch:  36 mini batch:  400   =====> Loss= 0.406870633\n",
      "Epoch:  36   =====> Loss= 0.364743419 ,   Train accuracy= 0.875 ,   Validation accuracy= 0.8406\n",
      "Epoch:  37 mini batch:  00   =====> Loss= 0.557136059\n",
      "Epoch:  37 mini batch:  100   =====> Loss= 0.293422759\n",
      "Epoch:  37 mini batch:  200   =====> Loss= 0.384108603\n",
      "Epoch:  37 mini batch:  300   =====> Loss= 0.314242065\n",
      "Epoch:  37 mini batch:  400   =====> Loss= 0.484342128\n",
      "Epoch:  37   =====> Loss= 0.358730780 ,   Train accuracy= 0.8984375 ,   Validation accuracy= 0.9068\n",
      "Epoch:  38 mini batch:  00   =====> Loss= 0.327036381\n",
      "Epoch:  38 mini batch:  100   =====> Loss= 0.321785808\n",
      "Epoch:  38 mini batch:  200   =====> Loss= 0.328065872\n",
      "Epoch:  38 mini batch:  300   =====> Loss= 0.374291390\n",
      "Epoch:  38 mini batch:  400   =====> Loss= 0.350181937\n",
      "Epoch:  38   =====> Loss= 0.349525235 ,   Train accuracy= 0.8828125 ,   Validation accuracy= 0.9026\n",
      "Epoch:  39 mini batch:  00   =====> Loss= 0.397650689\n",
      "Epoch:  39 mini batch:  100   =====> Loss= 0.216670185\n",
      "Epoch:  39 mini batch:  200   =====> Loss= 0.277996719\n",
      "Epoch:  39 mini batch:  300   =====> Loss= 0.277984023\n",
      "Epoch:  39 mini batch:  400   =====> Loss= 0.353203475\n",
      "Epoch:  39   =====> Loss= 0.350212397 ,   Train accuracy= 0.890625 ,   Validation accuracy= 0.9162\n",
      "Epoch:  40 mini batch:  00   =====> Loss= 0.409756482\n",
      "Epoch:  40 mini batch:  100   =====> Loss= 0.254147142\n",
      "Epoch:  40 mini batch:  200   =====> Loss= 0.235136747\n",
      "Epoch:  40 mini batch:  300   =====> Loss= 0.281121194\n",
      "Epoch:  40 mini batch:  400   =====> Loss= 0.358505219\n",
      "Epoch:  40   =====> Loss= 0.341644515 ,   Train accuracy= 0.9453125 ,   Validation accuracy= 0.919\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9107\n",
      "Time: 808.4979629516602 s\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNet-5_SGD\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNet-5_SGD\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "display_step = 100 #display loss every 10 mini-batch operations\n",
    "\n",
    "#reshape the inputs\n",
    "x_train=X_train.reshape(-1, 28, 28, 1)\n",
    "x_validation=X_validation.reshape(-1, 28, 28, 1)\n",
    "\n",
    "def train(init, sess, logs_path, n_epochs, batch_size, optimizer, cost, merged_summary_op):\n",
    "    # optimizer and cost are the same kinds of objects as in Section 1\n",
    "    # Train your model\n",
    "    # Launch the graph for training\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    saver=tf.train.Saver()\n",
    "    # Training cycle\n",
    "    start = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            #We shuffle the inputs at the first iteration on the batch\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            batch_xs=batch_xs.reshape(-1,28,28,1)\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            if i % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"mini batch: \", '%02d' %i, \"  =====> Loss=\", \"{:.9f}\".format(c))\n",
    "            \n",
    "        # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        #Display logs per epoch step\n",
    "        print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost),\n",
    "              \",   Train accuracy=\", acc.eval({x: batch_xs, y:batch_ys}),\n",
    "              \",   Validation accuracy=\", acc.eval({x: x_validation, y: y_validation}))\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start \n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "    # Save the model after training\n",
    "    saver.save(sess, 'MNIST_figures/model'+str(optimizer)[7:10])\n",
    "    \n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: X_test.reshape(-1,28,28,1), y: y_test}))\n",
    "    #display time of the training phase\n",
    "    print(\"Time:\", duration, \"s\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put your loss and accuracy curves here.\n",
    "\n",
    "![title](MNIST_figures/accSGD1.png)\n",
    "![title](MNIST_figures/lossSGD1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |         ...        |        ...          |       \n",
    "| Training Time        |         ...        |        ...          |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**Your answer:** ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01 mini batch:  00   =====> Loss= 19.967731476\n",
      "Epoch:  01 mini batch:  100   =====> Loss= 18.776060104\n",
      "Epoch:  01 mini batch:  200   =====> Loss= 18.318424225\n",
      "Epoch:  01 mini batch:  300   =====> Loss= 17.988945007\n",
      "Epoch:  01 mini batch:  400   =====> Loss= 17.629165649\n",
      "Epoch:  01   =====> Loss= 19.039522204 ,   Train accuracy= 0.1640625 ,   Validation accuracy= 0.1906\n",
      "Epoch:  02 mini batch:  00   =====> Loss= 19.248172760\n",
      "Epoch:  02 mini batch:  100   =====> Loss= 18.168834686\n",
      "Epoch:  02 mini batch:  200   =====> Loss= 16.551845551\n",
      "Epoch:  02 mini batch:  300   =====> Loss= 16.190050125\n",
      "Epoch:  02 mini batch:  400   =====> Loss= 17.629165649\n",
      "Epoch:  02   =====> Loss= 18.498232764 ,   Train accuracy= 0.203125 ,   Validation accuracy= 0.19\n",
      "Epoch:  03 mini batch:  00   =====> Loss= 19.453281403\n",
      "Epoch:  03 mini batch:  100   =====> Loss= 18.168834686\n",
      "Epoch:  03 mini batch:  200   =====> Loss= 16.010162354\n",
      "Epoch:  03 mini batch:  300   =====> Loss= 17.449275970\n",
      "Epoch:  03 mini batch:  400   =====> Loss= 19.372791290\n",
      "Epoch:  03   =====> Loss= 18.475382500 ,   Train accuracy= 0.1875 ,   Validation accuracy= 0.192\n",
      "Epoch:  04 mini batch:  00   =====> Loss= 19.068283081\n",
      "Epoch:  04 mini batch:  100   =====> Loss= 17.629165649\n",
      "Epoch:  04 mini batch:  200   =====> Loss= 17.208263397\n",
      "Epoch:  04 mini batch:  300   =====> Loss= 9.539603233\n",
      "Epoch:  04 mini batch:  400   =====> Loss= 8.994472504\n",
      "Epoch:  04   =====> Loss= 14.455889403 ,   Train accuracy= 0.5078125 ,   Validation accuracy= 0.5718\n",
      "Epoch:  05 mini batch:  00   =====> Loss= 10.253699303\n",
      "Epoch:  05 mini batch:  100   =====> Loss= 9.153368950\n",
      "Epoch:  05 mini batch:  200   =====> Loss= 10.613477707\n",
      "Epoch:  05 mini batch:  300   =====> Loss= 10.253947258\n",
      "Epoch:  05 mini batch:  400   =====> Loss= 8.923484802\n",
      "Epoch:  05   =====> Loss= 9.487822959 ,   Train accuracy= 0.5703125 ,   Validation accuracy= 0.5794\n",
      "Epoch:  06 mini batch:  00   =====> Loss= 10.613477707\n",
      "Epoch:  06 mini batch:  100   =====> Loss= 9.504898071\n",
      "Epoch:  06 mini batch:  200   =====> Loss= 10.254299164\n",
      "Epoch:  06 mini batch:  300   =====> Loss= 9.639226913\n",
      "Epoch:  06 mini batch:  400   =====> Loss= 9.939599991\n",
      "Epoch:  06   =====> Loss= 9.334221848 ,   Train accuracy= 0.5859375 ,   Validation accuracy= 0.5828\n",
      "Epoch:  07 mini batch:  00   =====> Loss= 10.406549454\n",
      "Epoch:  07 mini batch:  100   =====> Loss= 7.659399986\n",
      "Epoch:  07 mini batch:  200   =====> Loss= 6.858311176\n",
      "Epoch:  07 mini batch:  300   =====> Loss= 7.578663826\n",
      "Epoch:  07 mini batch:  400   =====> Loss= 5.033515453\n",
      "Epoch:  07   =====> Loss= 7.383783217 ,   Train accuracy= 0.734375 ,   Validation accuracy= 0.7676\n",
      "Epoch:  08 mini batch:  00   =====> Loss= 5.024599552\n",
      "Epoch:  08 mini batch:  100   =====> Loss= 5.640845299\n",
      "Epoch:  08 mini batch:  200   =====> Loss= 4.272289276\n",
      "Epoch:  08 mini batch:  300   =====> Loss= 3.680399418\n",
      "Epoch:  08 mini batch:  400   =====> Loss= 1.053285480\n",
      "Epoch:  08   =====> Loss= 3.601356226 ,   Train accuracy= 0.9296875 ,   Validation accuracy= 0.9284\n",
      "Epoch:  09 mini batch:  00   =====> Loss= 0.785891712\n",
      "Epoch:  09 mini batch:  100   =====> Loss= 0.291153491\n",
      "Epoch:  09 mini batch:  200   =====> Loss= 0.269902289\n",
      "Epoch:  09 mini batch:  300   =====> Loss= 0.093302704\n",
      "Epoch:  09 mini batch:  400   =====> Loss= 0.422767222\n",
      "Epoch:  09   =====> Loss= 0.446431142 ,   Train accuracy= 0.90625 ,   Validation accuracy= 0.9352\n",
      "Epoch:  10 mini batch:  00   =====> Loss= 0.262447834\n",
      "Epoch:  10 mini batch:  100   =====> Loss= 0.127407923\n",
      "Epoch:  10 mini batch:  200   =====> Loss= 0.185901254\n",
      "Epoch:  10 mini batch:  300   =====> Loss= 0.147673368\n",
      "Epoch:  10 mini batch:  400   =====> Loss= 0.177558124\n",
      "Epoch:  10   =====> Loss= 0.229631856 ,   Train accuracy= 0.9609375 ,   Validation accuracy= 0.939\n",
      "Epoch:  11 mini batch:  00   =====> Loss= 0.301099002\n",
      "Epoch:  11 mini batch:  100   =====> Loss= 0.104866475\n",
      "Epoch:  11 mini batch:  200   =====> Loss= 0.121029183\n",
      "Epoch:  11 mini batch:  300   =====> Loss= 0.037038479\n",
      "Epoch:  11 mini batch:  400   =====> Loss= 0.062837385\n",
      "Epoch:  11   =====> Loss= 0.167600835 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9614\n",
      "Epoch:  12 mini batch:  00   =====> Loss= 0.053917121\n",
      "Epoch:  12 mini batch:  100   =====> Loss= 0.118001245\n",
      "Epoch:  12 mini batch:  200   =====> Loss= 0.281040341\n",
      "Epoch:  12 mini batch:  300   =====> Loss= 0.272473842\n",
      "Epoch:  12 mini batch:  400   =====> Loss= 0.085324764\n",
      "Epoch:  12   =====> Loss= 0.131810597 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.962\n",
      "Epoch:  13 mini batch:  00   =====> Loss= 0.057729319\n",
      "Epoch:  13 mini batch:  100   =====> Loss= 0.143121794\n",
      "Epoch:  13 mini batch:  200   =====> Loss= 0.258756697\n",
      "Epoch:  13 mini batch:  300   =====> Loss= 0.053110518\n",
      "Epoch:  13 mini batch:  400   =====> Loss= 0.051504977\n",
      "Epoch:  13   =====> Loss= 0.110368402 ,   Train accuracy= 0.9609375 ,   Validation accuracy= 0.9666\n",
      "Epoch:  14 mini batch:  00   =====> Loss= 0.125824511\n",
      "Epoch:  14 mini batch:  100   =====> Loss= 0.073109478\n",
      "Epoch:  14 mini batch:  200   =====> Loss= 0.237197906\n",
      "Epoch:  14 mini batch:  300   =====> Loss= 0.037856508\n",
      "Epoch:  14 mini batch:  400   =====> Loss= 0.014879650\n",
      "Epoch:  14   =====> Loss= 0.094936196 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.97\n",
      "Epoch:  15 mini batch:  00   =====> Loss= 0.051915217\n",
      "Epoch:  15 mini batch:  100   =====> Loss= 0.040948924\n",
      "Epoch:  15 mini batch:  200   =====> Loss= 0.064064115\n",
      "Epoch:  15 mini batch:  300   =====> Loss= 0.088265166\n",
      "Epoch:  15 mini batch:  400   =====> Loss= 0.046881273\n",
      "Epoch:  15   =====> Loss= 0.083396095 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9722\n",
      "Epoch:  16 mini batch:  00   =====> Loss= 0.027843008\n",
      "Epoch:  16 mini batch:  100   =====> Loss= 0.099737972\n",
      "Epoch:  16 mini batch:  200   =====> Loss= 0.094743736\n",
      "Epoch:  16 mini batch:  300   =====> Loss= 0.012468765\n",
      "Epoch:  16 mini batch:  400   =====> Loss= 0.076570645\n",
      "Epoch:  16   =====> Loss= 0.073602522 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.9748\n",
      "Epoch:  17 mini batch:  00   =====> Loss= 0.040878173\n",
      "Epoch:  17 mini batch:  100   =====> Loss= 0.162378073\n",
      "Epoch:  17 mini batch:  200   =====> Loss= 0.077272512\n",
      "Epoch:  17 mini batch:  300   =====> Loss= 0.070996627\n",
      "Epoch:  17 mini batch:  400   =====> Loss= 0.112593487\n",
      "Epoch:  17   =====> Loss= 0.063701874 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9762\n",
      "Epoch:  18 mini batch:  00   =====> Loss= 0.013104804\n",
      "Epoch:  18 mini batch:  100   =====> Loss= 0.022742748\n",
      "Epoch:  18 mini batch:  200   =====> Loss= 0.015041617\n",
      "Epoch:  18 mini batch:  300   =====> Loss= 0.032165375\n",
      "Epoch:  18 mini batch:  400   =====> Loss= 0.017320883\n",
      "Epoch:  18   =====> Loss= 0.058919791 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9776\n",
      "Epoch:  19 mini batch:  00   =====> Loss= 0.094865285\n",
      "Epoch:  19 mini batch:  100   =====> Loss= 0.084195465\n",
      "Epoch:  19 mini batch:  200   =====> Loss= 0.018641846\n",
      "Epoch:  19 mini batch:  300   =====> Loss= 0.093995154\n",
      "Epoch:  19 mini batch:  400   =====> Loss= 0.106358096\n",
      "Epoch:  19   =====> Loss= 0.054492306 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9752\n",
      "Epoch:  20 mini batch:  00   =====> Loss= 0.069900349\n",
      "Epoch:  20 mini batch:  100   =====> Loss= 0.024786567\n",
      "Epoch:  20 mini batch:  200   =====> Loss= 0.069460049\n",
      "Epoch:  20 mini batch:  300   =====> Loss= 0.027945947\n",
      "Epoch:  20 mini batch:  400   =====> Loss= 0.053761952\n",
      "Epoch:  20   =====> Loss= 0.049442865 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.976\n",
      "Epoch:  21 mini batch:  00   =====> Loss= 0.012654425\n",
      "Epoch:  21 mini batch:  100   =====> Loss= 0.020273861\n",
      "Epoch:  21 mini batch:  200   =====> Loss= 0.070395783\n",
      "Epoch:  21 mini batch:  300   =====> Loss= 0.063305192\n",
      "Epoch:  21 mini batch:  400   =====> Loss= 0.046461053\n",
      "Epoch:  21   =====> Loss= 0.045617972 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9776\n",
      "Epoch:  22 mini batch:  00   =====> Loss= 0.016244769\n",
      "Epoch:  22 mini batch:  100   =====> Loss= 0.050647415\n",
      "Epoch:  22 mini batch:  200   =====> Loss= 0.093242541\n",
      "Epoch:  22 mini batch:  300   =====> Loss= 0.002915578\n",
      "Epoch:  22 mini batch:  400   =====> Loss= 0.162709087\n",
      "Epoch:  22   =====> Loss= 0.038863845 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9788\n",
      "Epoch:  23 mini batch:  00   =====> Loss= 0.252369851\n",
      "Epoch:  23 mini batch:  100   =====> Loss= 0.011383209\n",
      "Epoch:  23 mini batch:  200   =====> Loss= 0.097750515\n",
      "Epoch:  23 mini batch:  300   =====> Loss= 0.026390804\n",
      "Epoch:  23 mini batch:  400   =====> Loss= 0.049905524\n",
      "Epoch:  23   =====> Loss= 0.037560462 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.978\n",
      "Epoch:  24 mini batch:  00   =====> Loss= 0.243582740\n",
      "Epoch:  24 mini batch:  100   =====> Loss= 0.067569189\n",
      "Epoch:  24 mini batch:  200   =====> Loss= 0.021723727\n",
      "Epoch:  24 mini batch:  300   =====> Loss= 0.031981587\n",
      "Epoch:  24 mini batch:  400   =====> Loss= 0.002844470\n",
      "Epoch:  24   =====> Loss= 0.034854652 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.979\n",
      "Epoch:  25 mini batch:  00   =====> Loss= 0.022120113\n",
      "Epoch:  25 mini batch:  100   =====> Loss= 0.042869523\n",
      "Epoch:  25 mini batch:  200   =====> Loss= 0.014395724\n",
      "Epoch:  25 mini batch:  300   =====> Loss= 0.010365285\n",
      "Epoch:  25 mini batch:  400   =====> Loss= 0.071895413\n",
      "Epoch:  25   =====> Loss= 0.030807615 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9772\n",
      "Epoch:  26 mini batch:  00   =====> Loss= 0.003041042\n",
      "Epoch:  26 mini batch:  100   =====> Loss= 0.011375898\n",
      "Epoch:  26 mini batch:  200   =====> Loss= 0.018913835\n",
      "Epoch:  26 mini batch:  300   =====> Loss= 0.019520149\n",
      "Epoch:  26 mini batch:  400   =====> Loss= 0.013116049\n",
      "Epoch:  26   =====> Loss= 0.026057537 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9788\n",
      "Epoch:  27 mini batch:  00   =====> Loss= 0.014461128\n",
      "Epoch:  27 mini batch:  100   =====> Loss= 0.001847384\n",
      "Epoch:  27 mini batch:  200   =====> Loss= 0.072208807\n",
      "Epoch:  27 mini batch:  300   =====> Loss= 0.043556232\n",
      "Epoch:  27 mini batch:  400   =====> Loss= 0.006303054\n",
      "Epoch:  27   =====> Loss= 0.031337483 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9732\n",
      "Epoch:  28 mini batch:  00   =====> Loss= 0.073574603\n",
      "Epoch:  28 mini batch:  100   =====> Loss= 0.001433833\n",
      "Epoch:  28 mini batch:  200   =====> Loss= 0.018393362\n",
      "Epoch:  28 mini batch:  300   =====> Loss= 0.106520407\n",
      "Epoch:  28 mini batch:  400   =====> Loss= 0.006458297\n",
      "Epoch:  28   =====> Loss= 0.025349526 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.982\n",
      "Epoch:  29 mini batch:  00   =====> Loss= 0.009696017\n",
      "Epoch:  29 mini batch:  100   =====> Loss= 0.055143870\n",
      "Epoch:  29 mini batch:  200   =====> Loss= 0.030801941\n",
      "Epoch:  29 mini batch:  300   =====> Loss= 0.000623746\n",
      "Epoch:  29 mini batch:  400   =====> Loss= 0.002912705\n",
      "Epoch:  29   =====> Loss= 0.022350387 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9836\n",
      "Epoch:  30 mini batch:  00   =====> Loss= 0.024787160\n",
      "Epoch:  30 mini batch:  100   =====> Loss= 0.004170830\n",
      "Epoch:  30 mini batch:  200   =====> Loss= 0.000583929\n",
      "Epoch:  30 mini batch:  300   =====> Loss= 0.053528666\n",
      "Epoch:  30 mini batch:  400   =====> Loss= 0.003882068\n",
      "Epoch:  30   =====> Loss= 0.024631021 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9778\n",
      "Epoch:  31 mini batch:  00   =====> Loss= 0.000208864\n",
      "Epoch:  31 mini batch:  100   =====> Loss= 0.000156840\n",
      "Epoch:  31 mini batch:  200   =====> Loss= 0.041576535\n",
      "Epoch:  31 mini batch:  300   =====> Loss= 0.010045025\n",
      "Epoch:  31 mini batch:  400   =====> Loss= 0.002109146\n",
      "Epoch:  31   =====> Loss= 0.020739555 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9814\n",
      "Epoch:  32 mini batch:  00   =====> Loss= 0.060969606\n",
      "Epoch:  32 mini batch:  100   =====> Loss= 0.007584054\n",
      "Epoch:  32 mini batch:  200   =====> Loss= 0.004940216\n",
      "Epoch:  32 mini batch:  300   =====> Loss= 0.003050863\n",
      "Epoch:  32 mini batch:  400   =====> Loss= 0.005531525\n",
      "Epoch:  32   =====> Loss= 0.017778808 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9818\n",
      "Epoch:  33 mini batch:  00   =====> Loss= 0.000037163\n",
      "Epoch:  33 mini batch:  100   =====> Loss= 0.000259807\n",
      "Epoch:  33 mini batch:  200   =====> Loss= 0.206110060\n",
      "Epoch:  33 mini batch:  300   =====> Loss= 0.000454971\n",
      "Epoch:  33 mini batch:  400   =====> Loss= 0.000450540\n",
      "Epoch:  33   =====> Loss= 0.018026079 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9808\n",
      "Epoch:  34 mini batch:  00   =====> Loss= 0.030806987\n",
      "Epoch:  34 mini batch:  100   =====> Loss= 0.000979301\n",
      "Epoch:  34 mini batch:  200   =====> Loss= 0.182470232\n",
      "Epoch:  34 mini batch:  300   =====> Loss= 0.003059936\n",
      "Epoch:  34 mini batch:  400   =====> Loss= 0.038127180\n",
      "Epoch:  34   =====> Loss= 0.019632155 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9822\n",
      "Epoch:  35 mini batch:  00   =====> Loss= 0.072621673\n",
      "Epoch:  35 mini batch:  100   =====> Loss= 0.002384770\n",
      "Epoch:  35 mini batch:  200   =====> Loss= 0.000125477\n",
      "Epoch:  35 mini batch:  300   =====> Loss= 0.018240487\n",
      "Epoch:  35 mini batch:  400   =====> Loss= 0.002463668\n",
      "Epoch:  35   =====> Loss= 0.017902399 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.982\n",
      "Epoch:  36 mini batch:  00   =====> Loss= 0.040957972\n",
      "Epoch:  36 mini batch:  100   =====> Loss= 0.021947434\n",
      "Epoch:  36 mini batch:  200   =====> Loss= 0.000205661\n",
      "Epoch:  36 mini batch:  300   =====> Loss= 0.023107329\n",
      "Epoch:  36 mini batch:  400   =====> Loss= 0.001520034\n",
      "Epoch:  36   =====> Loss= 0.016777676 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9812\n",
      "Epoch:  37 mini batch:  00   =====> Loss= 0.000201717\n",
      "Epoch:  37 mini batch:  100   =====> Loss= 0.000122537\n",
      "Epoch:  37 mini batch:  200   =====> Loss= 0.033009991\n",
      "Epoch:  37 mini batch:  300   =====> Loss= 0.001403789\n",
      "Epoch:  37 mini batch:  400   =====> Loss= 0.000285906\n",
      "Epoch:  37   =====> Loss= 0.012802608 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9828\n",
      "Epoch:  38 mini batch:  00   =====> Loss= 0.006326898\n",
      "Epoch:  38 mini batch:  100   =====> Loss= 0.000060021\n",
      "Epoch:  38 mini batch:  200   =====> Loss= 0.000016887\n",
      "Epoch:  38 mini batch:  300   =====> Loss= 0.015692484\n",
      "Epoch:  38 mini batch:  400   =====> Loss= 0.000805459\n",
      "Epoch:  38   =====> Loss= 0.011397369 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9812\n",
      "Epoch:  39 mini batch:  00   =====> Loss= 0.001208477\n",
      "Epoch:  39 mini batch:  100   =====> Loss= 0.001201089\n",
      "Epoch:  39 mini batch:  200   =====> Loss= 0.000781176\n",
      "Epoch:  39 mini batch:  300   =====> Loss= 0.027972436\n",
      "Epoch:  39 mini batch:  400   =====> Loss= 0.000017081\n",
      "Epoch:  39   =====> Loss= 0.017312445 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9816\n",
      "Epoch:  40 mini batch:  00   =====> Loss= 0.002307639\n",
      "Epoch:  40 mini batch:  100   =====> Loss= 0.009646285\n",
      "Epoch:  40 mini batch:  200   =====> Loss= 0.000000721\n",
      "Epoch:  40 mini batch:  300   =====> Loss= 0.000156617\n",
      "Epoch:  40 mini batch:  400   =====> Loss= 0.185144484\n",
      "Epoch:  40   =====> Loss= 0.017816997 ,   Train accuracy= 1.0 ,   Validation accuracy= 0.9794\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9781\n",
      "Time: 878.3846790790558 s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# your implementation goes here\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"x\")\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name=\"y\")\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = LeNet5_Model(x) # The LeNet architecture we implemented previously\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('Adam'):\n",
    "    # Adam\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc=evaluate(pred,y)\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNet-5_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNet-5_Adam\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "###\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "x_train=X_train.reshape(-1, 28, 28, 1)\n",
    "x_validation=X_validation.reshape(-1, 28, 28, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](MNIST_figures/accAdam1.png)\n",
    "![title](MNIST_figures/lossAdam1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get:\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |         91,07%     |        97,81%       |       \n",
    "| Training Time        |         13'29''    |        14'38''      |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**Your answer:** The Adam Optimizer gives the best accuracy. When we look at the curves, it reaches more than 90% of accuracy after 7 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(image):    \n",
    "    # your implementation goes here\n",
    "    #2 convolution layers\n",
    "    conv = create_conv(image,5,6,\"SAME\")\n",
    "    conv = create_conv(conv,5,16,\"VALID\")\n",
    "    flat=flatten(conv)\n",
    "    \n",
    "    #add the dropout layer before the first fully connected layer\n",
    "    flat = tf.nn.dropout(flat, keep_prob = 0.75)\n",
    "\n",
    "    # First fully connected layer\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(int(flat.get_shape()[1]), 120)))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(flat, fc1_W) + fc1_b\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Second fully connected layer\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(int(fc1.get_shape()[1]), 84)))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2   = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Third fully connected layer\n",
    "    fc3_W = tf.Variable(tf.truncated_normal(shape=(int(fc2.get_shape()[1]), 10)))\n",
    "    fc3_b = tf.Variable(tf.zeros(10))\n",
    "    fc3   = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    logits = tf.nn.softmax(fc3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01 mini batch:  00   =====> Loss= 21.297061920\n",
      "Epoch:  01 mini batch:  100   =====> Loss= 15.928880692\n",
      "Epoch:  01 mini batch:  200   =====> Loss= 2.222682953\n",
      "Epoch:  01 mini batch:  300   =====> Loss= 2.317413807\n",
      "Epoch:  01 mini batch:  400   =====> Loss= 2.156584740\n",
      "Epoch:  01   =====> Loss= 6.254903066 ,   Train accuracy= 0.109375 ,   Validation accuracy= 0.1256\n",
      "Epoch:  02 mini batch:  00   =====> Loss= 2.376873255\n",
      "Epoch:  02 mini batch:  100   =====> Loss= 2.108033657\n",
      "Epoch:  02 mini batch:  200   =====> Loss= 2.124039650\n",
      "Epoch:  02 mini batch:  300   =====> Loss= 1.997534513\n",
      "Epoch:  02 mini batch:  400   =====> Loss= 1.894331932\n",
      "Epoch:  02   =====> Loss= 2.008781370 ,   Train accuracy= 0.46875 ,   Validation accuracy= 0.4266\n",
      "Epoch:  03 mini batch:  00   =====> Loss= 1.790112376\n",
      "Epoch:  03 mini batch:  100   =====> Loss= 1.747947454\n",
      "Epoch:  03 mini batch:  200   =====> Loss= 2.079729319\n",
      "Epoch:  03 mini batch:  300   =====> Loss= 1.622712731\n",
      "Epoch:  03 mini batch:  400   =====> Loss= 2.051100731\n",
      "Epoch:  03   =====> Loss= 1.794541536 ,   Train accuracy= 0.6015625 ,   Validation accuracy= 0.4934\n",
      "Epoch:  04 mini batch:  00   =====> Loss= 1.628947377\n",
      "Epoch:  04 mini batch:  100   =====> Loss= 1.860972285\n",
      "Epoch:  04 mini batch:  200   =====> Loss= 1.437299252\n",
      "Epoch:  04 mini batch:  300   =====> Loss= 1.520100832\n",
      "Epoch:  04 mini batch:  400   =====> Loss= 1.529502630\n",
      "Epoch:  04   =====> Loss= 1.671948474 ,   Train accuracy= 0.5546875 ,   Validation accuracy= 0.516\n",
      "Epoch:  05 mini batch:  00   =====> Loss= 1.592077017\n",
      "Epoch:  05 mini batch:  100   =====> Loss= 1.553267717\n",
      "Epoch:  05 mini batch:  200   =====> Loss= 1.547884345\n",
      "Epoch:  05 mini batch:  300   =====> Loss= 1.546373606\n",
      "Epoch:  05 mini batch:  400   =====> Loss= 1.779315472\n",
      "Epoch:  05   =====> Loss= 1.595616602 ,   Train accuracy= 0.5390625 ,   Validation accuracy= 0.5194\n",
      "Epoch:  06 mini batch:  00   =====> Loss= 1.588982582\n",
      "Epoch:  06 mini batch:  100   =====> Loss= 1.482263327\n",
      "Epoch:  06 mini batch:  200   =====> Loss= 1.482416868\n",
      "Epoch:  06 mini batch:  300   =====> Loss= 1.557015181\n",
      "Epoch:  06 mini batch:  400   =====> Loss= 1.442930222\n",
      "Epoch:  06   =====> Loss= 1.524149753 ,   Train accuracy= 0.65625 ,   Validation accuracy= 0.4942\n",
      "Epoch:  07 mini batch:  00   =====> Loss= 1.431963682\n",
      "Epoch:  07 mini batch:  100   =====> Loss= 1.448181629\n",
      "Epoch:  07 mini batch:  200   =====> Loss= 1.279268742\n",
      "Epoch:  07 mini batch:  300   =====> Loss= 1.421733379\n",
      "Epoch:  07 mini batch:  400   =====> Loss= 1.219771862\n",
      "Epoch:  07   =====> Loss= 1.476604458 ,   Train accuracy= 0.609375 ,   Validation accuracy= 0.5488\n",
      "Epoch:  08 mini batch:  00   =====> Loss= 1.259441137\n",
      "Epoch:  08 mini batch:  100   =====> Loss= 1.239052176\n",
      "Epoch:  08 mini batch:  200   =====> Loss= 1.617357373\n",
      "Epoch:  08 mini batch:  300   =====> Loss= 1.258692980\n",
      "Epoch:  08 mini batch:  400   =====> Loss= 1.483603716\n",
      "Epoch:  08   =====> Loss= 1.437184978 ,   Train accuracy= 0.234375 ,   Validation accuracy= 0.2748\n",
      "Epoch:  09 mini batch:  00   =====> Loss= 2.042140007\n",
      "Epoch:  09 mini batch:  100   =====> Loss= 1.293583155\n",
      "Epoch:  09 mini batch:  200   =====> Loss= 1.723229885\n",
      "Epoch:  09 mini batch:  300   =====> Loss= 1.208793044\n",
      "Epoch:  09 mini batch:  400   =====> Loss= 1.388158083\n",
      "Epoch:  09   =====> Loss= 1.412977492 ,   Train accuracy= 0.5703125 ,   Validation accuracy= 0.5318\n",
      "Epoch:  10 mini batch:  00   =====> Loss= 1.436634898\n",
      "Epoch:  10 mini batch:  100   =====> Loss= 1.169966459\n",
      "Epoch:  10 mini batch:  200   =====> Loss= 1.925088286\n",
      "Epoch:  10 mini batch:  300   =====> Loss= 1.125581026\n",
      "Epoch:  10 mini batch:  400   =====> Loss= 1.131199956\n",
      "Epoch:  10   =====> Loss= 1.367202543 ,   Train accuracy= 0.640625 ,   Validation accuracy= 0.5956\n",
      "Epoch:  11 mini batch:  00   =====> Loss= 1.292625666\n",
      "Epoch:  11 mini batch:  100   =====> Loss= 1.175619364\n",
      "Epoch:  11 mini batch:  200   =====> Loss= 1.299225211\n",
      "Epoch:  11 mini batch:  300   =====> Loss= 1.002095580\n",
      "Epoch:  11 mini batch:  400   =====> Loss= 1.091075778\n",
      "Epoch:  11   =====> Loss= 1.324091961 ,   Train accuracy= 0.671875 ,   Validation accuracy= 0.6106\n",
      "Epoch:  12 mini batch:  00   =====> Loss= 1.070008993\n",
      "Epoch:  12 mini batch:  100   =====> Loss= 1.444155931\n",
      "Epoch:  12 mini batch:  200   =====> Loss= 1.223869801\n",
      "Epoch:  12 mini batch:  300   =====> Loss= 1.056786418\n",
      "Epoch:  12 mini batch:  400   =====> Loss= 1.549732804\n",
      "Epoch:  12   =====> Loss= 1.295321168 ,   Train accuracy= 0.6484375 ,   Validation accuracy= 0.629\n",
      "Epoch:  13 mini batch:  00   =====> Loss= 1.238240242\n",
      "Epoch:  13 mini batch:  100   =====> Loss= 1.238308072\n",
      "Epoch:  13 mini batch:  200   =====> Loss= 1.652457714\n",
      "Epoch:  13 mini batch:  300   =====> Loss= 1.375823021\n",
      "Epoch:  13 mini batch:  400   =====> Loss= 1.325946808\n",
      "Epoch:  13   =====> Loss= 1.260876714 ,   Train accuracy= 0.609375 ,   Validation accuracy= 0.6056\n",
      "Epoch:  14 mini batch:  00   =====> Loss= 1.119276524\n",
      "Epoch:  14 mini batch:  100   =====> Loss= 1.222023964\n",
      "Epoch:  14 mini batch:  200   =====> Loss= 1.200891733\n",
      "Epoch:  14 mini batch:  300   =====> Loss= 1.218336940\n",
      "Epoch:  14 mini batch:  400   =====> Loss= 1.252037287\n",
      "Epoch:  14   =====> Loss= 1.237331934 ,   Train accuracy= 0.65625 ,   Validation accuracy= 0.6014\n",
      "Epoch:  15 mini batch:  00   =====> Loss= 1.153586626\n",
      "Epoch:  15 mini batch:  100   =====> Loss= 1.310854435\n",
      "Epoch:  15 mini batch:  200   =====> Loss= 1.221954465\n",
      "Epoch:  15 mini batch:  300   =====> Loss= 1.248665810\n",
      "Epoch:  15 mini batch:  400   =====> Loss= 0.998375654\n",
      "Epoch:  15   =====> Loss= 1.207313889 ,   Train accuracy= 0.484375 ,   Validation accuracy= 0.5928\n",
      "Epoch:  16 mini batch:  00   =====> Loss= 1.064930081\n",
      "Epoch:  16 mini batch:  100   =====> Loss= 1.207382679\n",
      "Epoch:  16 mini batch:  200   =====> Loss= 1.326627731\n",
      "Epoch:  16 mini batch:  300   =====> Loss= 1.100844264\n",
      "Epoch:  16 mini batch:  400   =====> Loss= 1.172222137\n",
      "Epoch:  16   =====> Loss= 1.176926360 ,   Train accuracy= 0.59375 ,   Validation accuracy= 0.6194\n",
      "Epoch:  17 mini batch:  00   =====> Loss= 1.248923063\n",
      "Epoch:  17 mini batch:  100   =====> Loss= 1.395756602\n",
      "Epoch:  17 mini batch:  200   =====> Loss= 1.186967373\n",
      "Epoch:  17 mini batch:  300   =====> Loss= 1.116249681\n",
      "Epoch:  17 mini batch:  400   =====> Loss= 0.967547119\n",
      "Epoch:  17   =====> Loss= 1.144160704 ,   Train accuracy= 0.6015625 ,   Validation accuracy= 0.617\n",
      "Epoch:  18 mini batch:  00   =====> Loss= 1.321838856\n",
      "Epoch:  18 mini batch:  100   =====> Loss= 1.189156771\n",
      "Epoch:  18 mini batch:  200   =====> Loss= 0.950544834\n",
      "Epoch:  18 mini batch:  300   =====> Loss= 0.870888233\n",
      "Epoch:  18 mini batch:  400   =====> Loss= 1.273715138\n",
      "Epoch:  18   =====> Loss= 1.128278928 ,   Train accuracy= 0.640625 ,   Validation accuracy= 0.6598\n",
      "Epoch:  19 mini batch:  00   =====> Loss= 1.201732755\n",
      "Epoch:  19 mini batch:  100   =====> Loss= 1.195388079\n",
      "Epoch:  19 mini batch:  200   =====> Loss= 1.192160964\n",
      "Epoch:  19 mini batch:  300   =====> Loss= 1.135067105\n",
      "Epoch:  19 mini batch:  400   =====> Loss= 0.884837985\n",
      "Epoch:  19   =====> Loss= 1.097120397 ,   Train accuracy= 0.609375 ,   Validation accuracy= 0.6614\n",
      "Epoch:  20 mini batch:  00   =====> Loss= 1.014182568\n",
      "Epoch:  20 mini batch:  100   =====> Loss= 1.052670956\n",
      "Epoch:  20 mini batch:  200   =====> Loss= 0.918717504\n",
      "Epoch:  20 mini batch:  300   =====> Loss= 1.316396952\n",
      "Epoch:  20 mini batch:  400   =====> Loss= 0.950990438\n",
      "Epoch:  20   =====> Loss= 1.063453786 ,   Train accuracy= 0.59375 ,   Validation accuracy= 0.6374\n",
      "Epoch:  21 mini batch:  00   =====> Loss= 1.082358718\n",
      "Epoch:  21 mini batch:  100   =====> Loss= 1.409741402\n",
      "Epoch:  21 mini batch:  200   =====> Loss= 0.801987410\n",
      "Epoch:  21 mini batch:  300   =====> Loss= 1.104272485\n",
      "Epoch:  21 mini batch:  400   =====> Loss= 1.140731812\n",
      "Epoch:  21   =====> Loss= 1.046547346 ,   Train accuracy= 0.6484375 ,   Validation accuracy= 0.6664\n",
      "Epoch:  22 mini batch:  00   =====> Loss= 1.120825052\n",
      "Epoch:  22 mini batch:  100   =====> Loss= 0.995844543\n",
      "Epoch:  22 mini batch:  200   =====> Loss= 0.934457242\n",
      "Epoch:  22 mini batch:  300   =====> Loss= 1.086549878\n",
      "Epoch:  22 mini batch:  400   =====> Loss= 0.905649781\n",
      "Epoch:  22   =====> Loss= 1.024710326 ,   Train accuracy= 0.609375 ,   Validation accuracy= 0.692\n",
      "Epoch:  23 mini batch:  00   =====> Loss= 0.827392936\n",
      "Epoch:  23 mini batch:  100   =====> Loss= 1.049839139\n",
      "Epoch:  23 mini batch:  200   =====> Loss= 1.094304681\n",
      "Epoch:  23 mini batch:  300   =====> Loss= 0.883853853\n",
      "Epoch:  23 mini batch:  400   =====> Loss= 0.955925882\n",
      "Epoch:  23   =====> Loss= 0.996878242 ,   Train accuracy= 0.7421875 ,   Validation accuracy= 0.6992\n",
      "Epoch:  24 mini batch:  00   =====> Loss= 0.963716626\n",
      "Epoch:  24 mini batch:  100   =====> Loss= 0.906127751\n",
      "Epoch:  24 mini batch:  200   =====> Loss= 0.989563584\n",
      "Epoch:  24 mini batch:  300   =====> Loss= 1.127825141\n",
      "Epoch:  24 mini batch:  400   =====> Loss= 0.793896258\n",
      "Epoch:  24   =====> Loss= 0.985098256 ,   Train accuracy= 0.75 ,   Validation accuracy= 0.698\n",
      "Epoch:  25 mini batch:  00   =====> Loss= 1.006342888\n",
      "Epoch:  25 mini batch:  100   =====> Loss= 0.964903831\n",
      "Epoch:  25 mini batch:  200   =====> Loss= 1.200566769\n",
      "Epoch:  25 mini batch:  300   =====> Loss= 0.730182528\n",
      "Epoch:  25 mini batch:  400   =====> Loss= 0.892168701\n",
      "Epoch:  25   =====> Loss= 0.966026524 ,   Train accuracy= 0.734375 ,   Validation accuracy= 0.7026\n",
      "Epoch:  26 mini batch:  00   =====> Loss= 0.881018043\n",
      "Epoch:  26 mini batch:  100   =====> Loss= 1.074837923\n",
      "Epoch:  26 mini batch:  200   =====> Loss= 1.092969894\n",
      "Epoch:  26 mini batch:  300   =====> Loss= 0.856592298\n",
      "Epoch:  26 mini batch:  400   =====> Loss= 1.009856939\n",
      "Epoch:  26   =====> Loss= 0.964800010 ,   Train accuracy= 0.7421875 ,   Validation accuracy= 0.6986\n",
      "Epoch:  27 mini batch:  00   =====> Loss= 0.619979978\n",
      "Epoch:  27 mini batch:  100   =====> Loss= 0.931688070\n",
      "Epoch:  27 mini batch:  200   =====> Loss= 1.058845878\n",
      "Epoch:  27 mini batch:  300   =====> Loss= 0.982925892\n",
      "Epoch:  27 mini batch:  400   =====> Loss= 0.880611718\n",
      "Epoch:  27   =====> Loss= 0.940509767 ,   Train accuracy= 0.7421875 ,   Validation accuracy= 0.7206\n",
      "Epoch:  28 mini batch:  00   =====> Loss= 0.632342160\n",
      "Epoch:  28 mini batch:  100   =====> Loss= 0.896598518\n",
      "Epoch:  28 mini batch:  200   =====> Loss= 0.959681094\n",
      "Epoch:  28 mini batch:  300   =====> Loss= 0.786081314\n",
      "Epoch:  28 mini batch:  400   =====> Loss= 0.902011752\n",
      "Epoch:  28   =====> Loss= 0.916020937 ,   Train accuracy= 0.6796875 ,   Validation accuracy= 0.6982\n",
      "Epoch:  29 mini batch:  00   =====> Loss= 0.785721540\n",
      "Epoch:  29 mini batch:  100   =====> Loss= 0.806802511\n",
      "Epoch:  29 mini batch:  200   =====> Loss= 0.912225366\n",
      "Epoch:  29 mini batch:  300   =====> Loss= 1.146913886\n",
      "Epoch:  29 mini batch:  400   =====> Loss= 0.927111745\n",
      "Epoch:  29   =====> Loss= 0.906877879 ,   Train accuracy= 0.625 ,   Validation accuracy= 0.6674\n",
      "Epoch:  30 mini batch:  00   =====> Loss= 1.021422267\n",
      "Epoch:  30 mini batch:  100   =====> Loss= 0.724385381\n",
      "Epoch:  30 mini batch:  200   =====> Loss= 0.932785928\n",
      "Epoch:  30 mini batch:  300   =====> Loss= 0.914394379\n",
      "Epoch:  30 mini batch:  400   =====> Loss= 1.033221960\n",
      "Epoch:  30   =====> Loss= 0.894056124 ,   Train accuracy= 0.734375 ,   Validation accuracy= 0.7074\n",
      "Epoch:  31 mini batch:  00   =====> Loss= 1.009130239\n",
      "Epoch:  31 mini batch:  100   =====> Loss= 0.767434478\n",
      "Epoch:  31 mini batch:  200   =====> Loss= 0.920954227\n",
      "Epoch:  31 mini batch:  300   =====> Loss= 0.673492372\n",
      "Epoch:  31 mini batch:  400   =====> Loss= 0.952912211\n",
      "Epoch:  31   =====> Loss= 0.892386479 ,   Train accuracy= 0.75 ,   Validation accuracy= 0.7446\n",
      "Epoch:  32 mini batch:  00   =====> Loss= 0.647069454\n",
      "Epoch:  32 mini batch:  100   =====> Loss= 0.853949845\n",
      "Epoch:  32 mini batch:  200   =====> Loss= 0.698656440\n",
      "Epoch:  32 mini batch:  300   =====> Loss= 0.770141482\n",
      "Epoch:  32 mini batch:  400   =====> Loss= 0.969824791\n",
      "Epoch:  32   =====> Loss= 0.864789221 ,   Train accuracy= 0.7421875 ,   Validation accuracy= 0.7314\n",
      "Epoch:  33 mini batch:  00   =====> Loss= 0.895219922\n",
      "Epoch:  33 mini batch:  100   =====> Loss= 1.035467148\n",
      "Epoch:  33 mini batch:  200   =====> Loss= 0.940919816\n",
      "Epoch:  33 mini batch:  300   =====> Loss= 0.740133703\n",
      "Epoch:  33 mini batch:  400   =====> Loss= 0.839152992\n",
      "Epoch:  33   =====> Loss= 0.861765064 ,   Train accuracy= 0.734375 ,   Validation accuracy= 0.7492\n",
      "Epoch:  34 mini batch:  00   =====> Loss= 0.960302830\n",
      "Epoch:  34 mini batch:  100   =====> Loss= 0.923012972\n",
      "Epoch:  34 mini batch:  200   =====> Loss= 1.009370685\n",
      "Epoch:  34 mini batch:  300   =====> Loss= 0.907789826\n",
      "Epoch:  34 mini batch:  400   =====> Loss= 0.881486177\n",
      "Epoch:  34   =====> Loss= 0.847492739 ,   Train accuracy= 0.7890625 ,   Validation accuracy= 0.7418\n",
      "Epoch:  35 mini batch:  00   =====> Loss= 0.843273997\n",
      "Epoch:  35 mini batch:  100   =====> Loss= 0.730250239\n",
      "Epoch:  35 mini batch:  200   =====> Loss= 1.026605248\n",
      "Epoch:  35 mini batch:  300   =====> Loss= 0.896391749\n",
      "Epoch:  35 mini batch:  400   =====> Loss= 0.841058552\n",
      "Epoch:  35   =====> Loss= 0.840285053 ,   Train accuracy= 0.8046875 ,   Validation accuracy= 0.7552\n",
      "Epoch:  36 mini batch:  00   =====> Loss= 0.730220377\n",
      "Epoch:  36 mini batch:  100   =====> Loss= 1.004535198\n",
      "Epoch:  36 mini batch:  200   =====> Loss= 0.684635997\n",
      "Epoch:  36 mini batch:  300   =====> Loss= 0.875910997\n",
      "Epoch:  36 mini batch:  400   =====> Loss= 0.703801274\n",
      "Epoch:  36   =====> Loss= 0.828247124 ,   Train accuracy= 0.71875 ,   Validation accuracy= 0.7402\n",
      "Epoch:  37 mini batch:  00   =====> Loss= 0.731915951\n",
      "Epoch:  37 mini batch:  100   =====> Loss= 0.670161605\n",
      "Epoch:  37 mini batch:  200   =====> Loss= 0.796391666\n",
      "Epoch:  37 mini batch:  300   =====> Loss= 0.897846043\n",
      "Epoch:  37 mini batch:  400   =====> Loss= 0.730238318\n",
      "Epoch:  37   =====> Loss= 0.818571538 ,   Train accuracy= 0.7578125 ,   Validation accuracy= 0.7358\n",
      "Epoch:  38 mini batch:  00   =====> Loss= 0.666913092\n",
      "Epoch:  38 mini batch:  100   =====> Loss= 0.950396538\n",
      "Epoch:  38 mini batch:  200   =====> Loss= 0.835114479\n",
      "Epoch:  38 mini batch:  300   =====> Loss= 0.789104700\n",
      "Epoch:  38 mini batch:  400   =====> Loss= 0.822344065\n",
      "Epoch:  38   =====> Loss= 0.809498415 ,   Train accuracy= 0.7578125 ,   Validation accuracy= 0.7552\n",
      "Epoch:  39 mini batch:  00   =====> Loss= 0.796616971\n",
      "Epoch:  39 mini batch:  100   =====> Loss= 0.620869935\n",
      "Epoch:  39 mini batch:  200   =====> Loss= 0.876414120\n",
      "Epoch:  39 mini batch:  300   =====> Loss= 0.549996257\n",
      "Epoch:  39 mini batch:  400   =====> Loss= 0.854412377\n",
      "Epoch:  39   =====> Loss= 0.834971866 ,   Train accuracy= 0.71875 ,   Validation accuracy= 0.712\n",
      "Epoch:  40 mini batch:  00   =====> Loss= 0.865071416\n",
      "Epoch:  40 mini batch:  100   =====> Loss= 1.011438608\n",
      "Epoch:  40 mini batch:  200   =====> Loss= 0.766645670\n",
      "Epoch:  40 mini batch:  300   =====> Loss= 0.723121166\n",
      "Epoch:  40 mini batch:  400   =====> Loss= 1.003907919\n",
      "Epoch:  40   =====> Loss= 0.911403249 ,   Train accuracy= 0.78125 ,   Validation accuracy= 0.7198\n",
      "Optimization Finished!\n",
      "Accuracy: 0.7228\n",
      "Time: 826.9381904602051 s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# your implementation goes here\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"x\")\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name=\"y\")\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred =  LeNet5_Model_Dropout(x) # The LeNet architecture with dropout\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc=evaluate(pred,y)\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNetDropout-5_SGD\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNetDropout-5_SGD\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "###\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "x_train=X_train.reshape(-1, 28, 28, 1)\n",
    "x_validation=X_validation.reshape(-1, 28, 28, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](MNIST_figures/accSGD2.png)\n",
    "![title](MNIST_figures/lossSGD2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01 mini batch:  00   =====> Loss= 21.814531326\n",
      "Epoch:  01 mini batch:  100   =====> Loss= 15.481681824\n",
      "Epoch:  01 mini batch:  200   =====> Loss= 8.949636459\n",
      "Epoch:  01 mini batch:  300   =====> Loss= 2.127032757\n",
      "Epoch:  01 mini batch:  400   =====> Loss= 1.757178783\n",
      "Epoch:  01   =====> Loss= 8.220158888 ,   Train accuracy= 0.4921875 ,   Validation accuracy= 0.5426\n",
      "Epoch:  02 mini batch:  00   =====> Loss= 1.193530560\n",
      "Epoch:  02 mini batch:  100   =====> Loss= 1.106262684\n",
      "Epoch:  02 mini batch:  200   =====> Loss= 1.277941942\n",
      "Epoch:  02 mini batch:  300   =====> Loss= 0.913125813\n",
      "Epoch:  02 mini batch:  400   =====> Loss= 0.625115931\n",
      "Epoch:  02   =====> Loss= 0.982291756 ,   Train accuracy= 0.71875 ,   Validation accuracy= 0.7886\n",
      "Epoch:  03 mini batch:  00   =====> Loss= 0.596485198\n",
      "Epoch:  03 mini batch:  100   =====> Loss= 0.488438070\n",
      "Epoch:  03 mini batch:  200   =====> Loss= 0.784871817\n",
      "Epoch:  03 mini batch:  300   =====> Loss= 0.344962239\n",
      "Epoch:  03 mini batch:  400   =====> Loss= 0.428810149\n",
      "Epoch:  03   =====> Loss= 0.560642689 ,   Train accuracy= 0.8359375 ,   Validation accuracy= 0.8684\n",
      "Epoch:  04 mini batch:  00   =====> Loss= 0.429839551\n",
      "Epoch:  04 mini batch:  100   =====> Loss= 0.421249926\n",
      "Epoch:  04 mini batch:  200   =====> Loss= 0.343507409\n",
      "Epoch:  04 mini batch:  300   =====> Loss= 0.246926859\n",
      "Epoch:  04 mini batch:  400   =====> Loss= 0.449149787\n",
      "Epoch:  04   =====> Loss= 0.392931758 ,   Train accuracy= 0.8984375 ,   Validation accuracy= 0.8894\n",
      "Epoch:  05 mini batch:  00   =====> Loss= 0.377289772\n",
      "Epoch:  05 mini batch:  100   =====> Loss= 0.350216448\n",
      "Epoch:  05 mini batch:  200   =====> Loss= 0.354221046\n",
      "Epoch:  05 mini batch:  300   =====> Loss= 0.143929407\n",
      "Epoch:  05 mini batch:  400   =====> Loss= 0.452484369\n",
      "Epoch:  05   =====> Loss= 0.315475472 ,   Train accuracy= 0.921875 ,   Validation accuracy= 0.9154\n",
      "Epoch:  06 mini batch:  00   =====> Loss= 0.215815440\n",
      "Epoch:  06 mini batch:  100   =====> Loss= 0.397223592\n",
      "Epoch:  06 mini batch:  200   =====> Loss= 0.318052232\n",
      "Epoch:  06 mini batch:  300   =====> Loss= 0.149780706\n",
      "Epoch:  06 mini batch:  400   =====> Loss= 0.370646566\n",
      "Epoch:  06   =====> Loss= 0.260757349 ,   Train accuracy= 0.9453125 ,   Validation accuracy= 0.922\n",
      "Epoch:  07 mini batch:  00   =====> Loss= 0.195597857\n",
      "Epoch:  07 mini batch:  100   =====> Loss= 0.312731266\n",
      "Epoch:  07 mini batch:  200   =====> Loss= 0.182474017\n",
      "Epoch:  07 mini batch:  300   =====> Loss= 0.175551742\n",
      "Epoch:  07 mini batch:  400   =====> Loss= 0.244054049\n",
      "Epoch:  07   =====> Loss= 0.229944875 ,   Train accuracy= 0.9375 ,   Validation accuracy= 0.9324\n",
      "Epoch:  08 mini batch:  00   =====> Loss= 0.246530712\n",
      "Epoch:  08 mini batch:  100   =====> Loss= 0.229608178\n",
      "Epoch:  08 mini batch:  200   =====> Loss= 0.163362250\n",
      "Epoch:  08 mini batch:  300   =====> Loss= 0.183829919\n",
      "Epoch:  08 mini batch:  400   =====> Loss= 0.180343583\n",
      "Epoch:  08   =====> Loss= 0.197989812 ,   Train accuracy= 0.9453125 ,   Validation accuracy= 0.941\n",
      "Epoch:  09 mini batch:  00   =====> Loss= 0.283410490\n",
      "Epoch:  09 mini batch:  100   =====> Loss= 0.230207473\n",
      "Epoch:  09 mini batch:  200   =====> Loss= 0.217307806\n",
      "Epoch:  09 mini batch:  300   =====> Loss= 0.228812292\n",
      "Epoch:  09 mini batch:  400   =====> Loss= 0.123176947\n",
      "Epoch:  09   =====> Loss= 0.179483714 ,   Train accuracy= 0.9296875 ,   Validation accuracy= 0.9492\n",
      "Epoch:  10 mini batch:  00   =====> Loss= 0.241226494\n",
      "Epoch:  10 mini batch:  100   =====> Loss= 0.246213198\n",
      "Epoch:  10 mini batch:  200   =====> Loss= 0.129729226\n",
      "Epoch:  10 mini batch:  300   =====> Loss= 0.144059703\n",
      "Epoch:  10 mini batch:  400   =====> Loss= 0.208205640\n",
      "Epoch:  10   =====> Loss= 0.164544395 ,   Train accuracy= 0.953125 ,   Validation accuracy= 0.9494\n",
      "Epoch:  11 mini batch:  00   =====> Loss= 0.099296860\n",
      "Epoch:  11 mini batch:  100   =====> Loss= 0.140329540\n",
      "Epoch:  11 mini batch:  200   =====> Loss= 0.189989358\n",
      "Epoch:  11 mini batch:  300   =====> Loss= 0.110024780\n",
      "Epoch:  11 mini batch:  400   =====> Loss= 0.093114674\n",
      "Epoch:  11   =====> Loss= 0.148358022 ,   Train accuracy= 0.9453125 ,   Validation accuracy= 0.9518\n",
      "Epoch:  12 mini batch:  00   =====> Loss= 0.102171123\n",
      "Epoch:  12 mini batch:  100   =====> Loss= 0.174644291\n",
      "Epoch:  12 mini batch:  200   =====> Loss= 0.126322150\n",
      "Epoch:  12 mini batch:  300   =====> Loss= 0.126040056\n",
      "Epoch:  12 mini batch:  400   =====> Loss= 0.100507617\n",
      "Epoch:  12   =====> Loss= 0.136798130 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.954\n",
      "Epoch:  13 mini batch:  00   =====> Loss= 0.106946275\n",
      "Epoch:  13 mini batch:  100   =====> Loss= 0.075966880\n",
      "Epoch:  13 mini batch:  200   =====> Loss= 0.032013569\n",
      "Epoch:  13 mini batch:  300   =====> Loss= 0.102473721\n",
      "Epoch:  13 mini batch:  400   =====> Loss= 0.140943095\n",
      "Epoch:  13   =====> Loss= 0.128581059 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.9588\n",
      "Epoch:  14 mini batch:  00   =====> Loss= 0.146713123\n",
      "Epoch:  14 mini batch:  100   =====> Loss= 0.201629058\n",
      "Epoch:  14 mini batch:  200   =====> Loss= 0.176153556\n",
      "Epoch:  14 mini batch:  300   =====> Loss= 0.118146569\n",
      "Epoch:  14 mini batch:  400   =====> Loss= 0.091240004\n",
      "Epoch:  14   =====> Loss= 0.116751623 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.9676\n",
      "Epoch:  15 mini batch:  00   =====> Loss= 0.080892101\n",
      "Epoch:  15 mini batch:  100   =====> Loss= 0.185322553\n",
      "Epoch:  15 mini batch:  200   =====> Loss= 0.109960653\n",
      "Epoch:  15 mini batch:  300   =====> Loss= 0.074703529\n",
      "Epoch:  15 mini batch:  400   =====> Loss= 0.077386521\n",
      "Epoch:  15   =====> Loss= 0.109376614 ,   Train accuracy= 0.9609375 ,   Validation accuracy= 0.9632\n",
      "Epoch:  16 mini batch:  00   =====> Loss= 0.132100731\n",
      "Epoch:  16 mini batch:  100   =====> Loss= 0.094628982\n",
      "Epoch:  16 mini batch:  200   =====> Loss= 0.067777216\n",
      "Epoch:  16 mini batch:  300   =====> Loss= 0.107196882\n",
      "Epoch:  16 mini batch:  400   =====> Loss= 0.097237527\n",
      "Epoch:  16   =====> Loss= 0.110308180 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.9668\n",
      "Epoch:  17 mini batch:  00   =====> Loss= 0.055423915\n",
      "Epoch:  17 mini batch:  100   =====> Loss= 0.067712985\n",
      "Epoch:  17 mini batch:  200   =====> Loss= 0.039385319\n",
      "Epoch:  17 mini batch:  300   =====> Loss= 0.131696403\n",
      "Epoch:  17 mini batch:  400   =====> Loss= 0.101502433\n",
      "Epoch:  17   =====> Loss= 0.101321977 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9674\n",
      "Epoch:  18 mini batch:  00   =====> Loss= 0.105493203\n",
      "Epoch:  18 mini batch:  100   =====> Loss= 0.088086143\n",
      "Epoch:  18 mini batch:  200   =====> Loss= 0.041183196\n",
      "Epoch:  18 mini batch:  300   =====> Loss= 0.120328277\n",
      "Epoch:  18 mini batch:  400   =====> Loss= 0.185328111\n",
      "Epoch:  18   =====> Loss= 0.094212864 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9662\n",
      "Epoch:  19 mini batch:  00   =====> Loss= 0.066884786\n",
      "Epoch:  19 mini batch:  100   =====> Loss= 0.068855435\n",
      "Epoch:  19 mini batch:  200   =====> Loss= 0.075251527\n",
      "Epoch:  19 mini batch:  300   =====> Loss= 0.128589511\n",
      "Epoch:  19 mini batch:  400   =====> Loss= 0.151389152\n",
      "Epoch:  19   =====> Loss= 0.096322051 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9704\n",
      "Epoch:  20 mini batch:  00   =====> Loss= 0.030186616\n",
      "Epoch:  20 mini batch:  100   =====> Loss= 0.052535843\n",
      "Epoch:  20 mini batch:  200   =====> Loss= 0.078709006\n",
      "Epoch:  20 mini batch:  300   =====> Loss= 0.055066776\n",
      "Epoch:  20 mini batch:  400   =====> Loss= 0.035498768\n",
      "Epoch:  20   =====> Loss= 0.090294427 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9718\n",
      "Epoch:  21 mini batch:  00   =====> Loss= 0.131504685\n",
      "Epoch:  21 mini batch:  100   =====> Loss= 0.032800622\n",
      "Epoch:  21 mini batch:  200   =====> Loss= 0.036851738\n",
      "Epoch:  21 mini batch:  300   =====> Loss= 0.109737426\n",
      "Epoch:  21 mini batch:  400   =====> Loss= 0.022987003\n",
      "Epoch:  21   =====> Loss= 0.082041932 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.971\n",
      "Epoch:  22 mini batch:  00   =====> Loss= 0.053458333\n",
      "Epoch:  22 mini batch:  100   =====> Loss= 0.143998295\n",
      "Epoch:  22 mini batch:  200   =====> Loss= 0.071952142\n",
      "Epoch:  22 mini batch:  300   =====> Loss= 0.058161907\n",
      "Epoch:  22 mini batch:  400   =====> Loss= 0.033799727\n",
      "Epoch:  22   =====> Loss= 0.082462619 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.972\n",
      "Epoch:  23 mini batch:  00   =====> Loss= 0.030182445\n",
      "Epoch:  23 mini batch:  100   =====> Loss= 0.029103346\n",
      "Epoch:  23 mini batch:  200   =====> Loss= 0.083701104\n",
      "Epoch:  23 mini batch:  300   =====> Loss= 0.101636730\n",
      "Epoch:  23 mini batch:  400   =====> Loss= 0.010951854\n",
      "Epoch:  23   =====> Loss= 0.076596375 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.9762\n",
      "Epoch:  24 mini batch:  00   =====> Loss= 0.070119426\n",
      "Epoch:  24 mini batch:  100   =====> Loss= 0.019400392\n",
      "Epoch:  24 mini batch:  200   =====> Loss= 0.145645455\n",
      "Epoch:  24 mini batch:  300   =====> Loss= 0.097714335\n",
      "Epoch:  24 mini batch:  400   =====> Loss= 0.049879752\n",
      "Epoch:  24   =====> Loss= 0.076002815 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9752\n",
      "Epoch:  25 mini batch:  00   =====> Loss= 0.068447717\n",
      "Epoch:  25 mini batch:  100   =====> Loss= 0.008514110\n",
      "Epoch:  25 mini batch:  200   =====> Loss= 0.081067227\n",
      "Epoch:  25 mini batch:  300   =====> Loss= 0.076363571\n",
      "Epoch:  25 mini batch:  400   =====> Loss= 0.123466648\n",
      "Epoch:  25   =====> Loss= 0.074300642 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9736\n",
      "Epoch:  26 mini batch:  00   =====> Loss= 0.033168495\n",
      "Epoch:  26 mini batch:  100   =====> Loss= 0.043625154\n",
      "Epoch:  26 mini batch:  200   =====> Loss= 0.080804303\n",
      "Epoch:  26 mini batch:  300   =====> Loss= 0.125181139\n",
      "Epoch:  26 mini batch:  400   =====> Loss= 0.046091206\n",
      "Epoch:  26   =====> Loss= 0.069701553 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9764\n",
      "Epoch:  27 mini batch:  00   =====> Loss= 0.064900845\n",
      "Epoch:  27 mini batch:  100   =====> Loss= 0.031509802\n",
      "Epoch:  27 mini batch:  200   =====> Loss= 0.033003084\n",
      "Epoch:  27 mini batch:  300   =====> Loss= 0.072055534\n",
      "Epoch:  27 mini batch:  400   =====> Loss= 0.040480144\n",
      "Epoch:  27   =====> Loss= 0.071145698 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9734\n",
      "Epoch:  28 mini batch:  00   =====> Loss= 0.058814287\n",
      "Epoch:  28 mini batch:  100   =====> Loss= 0.124427639\n",
      "Epoch:  28 mini batch:  200   =====> Loss= 0.028378651\n",
      "Epoch:  28 mini batch:  300   =====> Loss= 0.026442612\n",
      "Epoch:  28 mini batch:  400   =====> Loss= 0.026533131\n",
      "Epoch:  28   =====> Loss= 0.073125830 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.974\n",
      "Epoch:  29 mini batch:  00   =====> Loss= 0.057432454\n",
      "Epoch:  29 mini batch:  100   =====> Loss= 0.249532595\n",
      "Epoch:  29 mini batch:  200   =====> Loss= 0.049825311\n",
      "Epoch:  29 mini batch:  300   =====> Loss= 0.041789491\n",
      "Epoch:  29 mini batch:  400   =====> Loss= 0.130103990\n",
      "Epoch:  29   =====> Loss= 0.065832093 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.9746\n",
      "Epoch:  30 mini batch:  00   =====> Loss= 0.057758696\n",
      "Epoch:  30 mini batch:  100   =====> Loss= 0.193070114\n",
      "Epoch:  30 mini batch:  200   =====> Loss= 0.023971327\n",
      "Epoch:  30 mini batch:  300   =====> Loss= 0.035889894\n",
      "Epoch:  30 mini batch:  400   =====> Loss= 0.112330459\n",
      "Epoch:  30   =====> Loss= 0.067297085 ,   Train accuracy= 0.9609375 ,   Validation accuracy= 0.9756\n",
      "Epoch:  31 mini batch:  00   =====> Loss= 0.038156703\n",
      "Epoch:  31 mini batch:  100   =====> Loss= 0.117927492\n",
      "Epoch:  31 mini batch:  200   =====> Loss= 0.089114659\n",
      "Epoch:  31 mini batch:  300   =====> Loss= 0.068079427\n",
      "Epoch:  31 mini batch:  400   =====> Loss= 0.055755086\n",
      "Epoch:  31   =====> Loss= 0.066409248 ,   Train accuracy= 0.96875 ,   Validation accuracy= 0.9758\n",
      "Epoch:  32 mini batch:  00   =====> Loss= 0.072022244\n",
      "Epoch:  32 mini batch:  100   =====> Loss= 0.024754036\n",
      "Epoch:  32 mini batch:  200   =====> Loss= 0.043793887\n",
      "Epoch:  32 mini batch:  300   =====> Loss= 0.050614741\n",
      "Epoch:  32 mini batch:  400   =====> Loss= 0.058439102\n",
      "Epoch:  32   =====> Loss= 0.064055801 ,   Train accuracy= 0.9609375 ,   Validation accuracy= 0.9768\n",
      "Epoch:  33 mini batch:  00   =====> Loss= 0.049495015\n",
      "Epoch:  33 mini batch:  100   =====> Loss= 0.140369654\n",
      "Epoch:  33 mini batch:  200   =====> Loss= 0.051057715\n",
      "Epoch:  33 mini batch:  300   =====> Loss= 0.030280301\n",
      "Epoch:  33 mini batch:  400   =====> Loss= 0.129261151\n",
      "Epoch:  33   =====> Loss= 0.061597078 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9802\n",
      "Epoch:  34 mini batch:  00   =====> Loss= 0.033382397\n",
      "Epoch:  34 mini batch:  100   =====> Loss= 0.089852117\n",
      "Epoch:  34 mini batch:  200   =====> Loss= 0.046396580\n",
      "Epoch:  34 mini batch:  300   =====> Loss= 0.097098731\n",
      "Epoch:  34 mini batch:  400   =====> Loss= 0.051785715\n",
      "Epoch:  34   =====> Loss= 0.060374358 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9748\n",
      "Epoch:  35 mini batch:  00   =====> Loss= 0.075947411\n",
      "Epoch:  35 mini batch:  100   =====> Loss= 0.015965942\n",
      "Epoch:  35 mini batch:  200   =====> Loss= 0.081891254\n",
      "Epoch:  35 mini batch:  300   =====> Loss= 0.037707921\n",
      "Epoch:  35 mini batch:  400   =====> Loss= 0.075891897\n",
      "Epoch:  35   =====> Loss= 0.058687608 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9786\n",
      "Epoch:  36 mini batch:  00   =====> Loss= 0.023000799\n",
      "Epoch:  36 mini batch:  100   =====> Loss= 0.061682224\n",
      "Epoch:  36 mini batch:  200   =====> Loss= 0.015945192\n",
      "Epoch:  36 mini batch:  300   =====> Loss= 0.033381656\n",
      "Epoch:  36 mini batch:  400   =====> Loss= 0.055364043\n",
      "Epoch:  36   =====> Loss= 0.058614576 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9792\n",
      "Epoch:  37 mini batch:  00   =====> Loss= 0.043972589\n",
      "Epoch:  37 mini batch:  100   =====> Loss= 0.067274734\n",
      "Epoch:  37 mini batch:  200   =====> Loss= 0.049513683\n",
      "Epoch:  37 mini batch:  300   =====> Loss= 0.081510305\n",
      "Epoch:  37 mini batch:  400   =====> Loss= 0.062112466\n",
      "Epoch:  37   =====> Loss= 0.056695554 ,   Train accuracy= 0.9765625 ,   Validation accuracy= 0.9768\n",
      "Epoch:  38 mini batch:  00   =====> Loss= 0.054093160\n",
      "Epoch:  38 mini batch:  100   =====> Loss= 0.058326520\n",
      "Epoch:  38 mini batch:  200   =====> Loss= 0.037498556\n",
      "Epoch:  38 mini batch:  300   =====> Loss= 0.017416399\n",
      "Epoch:  38 mini batch:  400   =====> Loss= 0.015797330\n",
      "Epoch:  38   =====> Loss= 0.053918969 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9778\n",
      "Epoch:  39 mini batch:  00   =====> Loss= 0.062314022\n",
      "Epoch:  39 mini batch:  100   =====> Loss= 0.012687976\n",
      "Epoch:  39 mini batch:  200   =====> Loss= 0.048604950\n",
      "Epoch:  39 mini batch:  300   =====> Loss= 0.084093675\n",
      "Epoch:  39 mini batch:  400   =====> Loss= 0.035697285\n",
      "Epoch:  39   =====> Loss= 0.054952113 ,   Train accuracy= 0.984375 ,   Validation accuracy= 0.98\n",
      "Epoch:  40 mini batch:  00   =====> Loss= 0.095185302\n",
      "Epoch:  40 mini batch:  100   =====> Loss= 0.099834003\n",
      "Epoch:  40 mini batch:  200   =====> Loss= 0.082530931\n",
      "Epoch:  40 mini batch:  300   =====> Loss= 0.086204693\n",
      "Epoch:  40 mini batch:  400   =====> Loss= 0.048158381\n",
      "Epoch:  40   =====> Loss= 0.054791163 ,   Train accuracy= 0.9921875 ,   Validation accuracy= 0.9796\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9795\n",
      "Time: 846.8850231170654 s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# your implementation goes here\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"x\")\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name=\"y\")\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred =  LeNet5_Model_Dropout(x) # The LeNet architecture we implemented previously\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('Adam'):\n",
    "    # Adam\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc=evaluate(pred,y)\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_LeNetDropout-5_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_LeNetDropout-5_Adam\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "###\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "x_train=X_train.reshape(-1, 28, 28, 1)\n",
    "x_validation=X_validation.reshape(-1, 28, 28, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](MNIST_figures/accAdam2.png)\n",
    "![title](MNIST_figures/lossAdam2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy achieved on testing data:** ...\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |         72,28%     |        97,95%       |       \n",
    "| Training Time        |         13'47''    |        14'06''      |  \n",
    "\n",
    "\n",
    "Usually, dropout is used to avoid overfitting. Here, we see that we don't need it as our result are not better than without.<br>\n",
    "With Dropout, the result for the gradient descent is worst than without.<br>\n",
    "The Adam Optimizer still provides better results. The results for the Adam Optimizer don't differ a lot from the model with the Dropout layer and without."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
